B. Perform logistic regression analysis.
Logistic regression is a classification method built on the same concept as linear
regression. With linear regression, we take linear combination of explanatory variables
plus an intercept term to arrive at a prediction.
In this example we will use a logistic regression model to predict survival.
Program Code:
import os
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn import linear_model
from sklearn import preprocessing
from sklearn import metrics
matplotlib.style.use('ggplot')
plt.figure(figsize=(9,9))
def sigmoid(t): # Define the sigmoid function
 return (1/(1 + np.e**(-t)))
plot_range = np.arange(-6, 6, 0.1)
y_values = sigmoid(plot_range)
# Plot curve
plt.plot(plot_range, # X-axis range
 y_values, # Predicted values
 color="red")
titanic_train = pd.read_csv("titanic_train.csv") # Read the data
char_cabin = titanic_train["Cabin"].astype(str) # Convert cabin to str
new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter
titanic_train["Cabin"] = pd.Categorical(new_Cabin) # Save the new cabin var
# Impute median Age for NA Age values
new_age_var = np.where(titanic_train["Age"].isnull(), # Logical check
 28, # Value if check is true
PSIT1P1~~~~~ Research in Computing Practical
M. Sc. [Information Technology] SEMESTER ~ I Teacher’s Reference Manual
60
 titanic_train["Age"]) # Value if check is false
titanic_train["Age"] = new_age_var
label_encoder = preprocessing.LabelEncoder()
# Convert Sex variable to numeric
encoded_sex = label_encoder.fit_transform(titanic_train["Sex"])
# Initialize logistic regression model
log_model = linear_model.LogisticRegression()
# Train the model
log_model.fit(X = pd.DataFrame(encoded_sex),
 y = titanic_train["Survived"])
# Check trained model intercept
print(log_model.intercept_)
# Check trained model coefficients
print(log_model.coef_)
# Make predictions
preds = log_model.predict_proba(X= pd.DataFrame(encoded_sex))
preds = pd.DataFrame(preds)
preds.columns = ["Death_prob", "Survival_prob"]
# Generate table of predictions vs Sex
pd.crosstab(titanic_train["Sex"], preds.ix[:, "Survival_prob"])
# Convert more variables to numeric
encoded_class = label_encoder.fit_transform(titanic_train["Pclass"])
encoded_cabin = label_encoder.fit_transform(titanic_train["Cabin"])
train_features = pd.DataFrame([encoded_class,
 encoded_cabin,
 encoded_sex,
 titanic_train["Age"]]).T
# Initialize logistic regression model
log_model = linear_model.LogisticRegression()
PSIT1P1~~~~~ Research in Computing Practical
M. Sc. [Information Technology] SEMESTER ~ I Teacher’s Reference Manual
61
# Train the model
log_model.fit(X = train_features ,
 y = titanic_train["Survived"])
# Check trained model intercept
print(log_model.intercept_)
# Check trained model coefficients
print(log_model.coef_)
# Make predictions
preds = log_model.predict(X= train_features)
# Generate table of predictions vs actual
pd.crosstab(preds,titanic_train["Survived"])
log_model.score(X = train_features ,
 y = titanic_train["Survived"])
metrics.confusion_matrix(y_true=titanic_train["Survived"], # True labels
 y_pred=preds) # Predicted labels
# View summary of common classification metrics
print(metrics.classification_report(y_true=titanic_train["Survived"],
 y_pred=preds) )
# Read and prepare test data
titanic_test = pd.read_csv("titanic_test.csv") # Read the data
char_cabin = titanic_test["Cabin"].astype(str) # Convert cabin to str
new_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter
titanic_test["Cabin"] = pd.Categorical(new_Cabin) # Save the new cabin var
# Impute median Age for NA Age values
new_age_var = np.where(titanic_test["Age"].isnull(), # Logical check
 28, # Value if check is true
 titanic_test["Age"]) # Value if check is false
PSIT1P1~~~~~ Research in Computing Practical
M. Sc. [Information Technology] SEMESTER ~ I Teacher’s Reference Manual
62
titanic_test["Age"] = new_age_var
# Convert test variables to match model features
encoded_sex = label_encoder.fit_transform(titanic_test["Sex"])
encoded_class = label_encoder.fit_transform(titanic_test["Pclass"])
encoded_cabin = label_encoder.fit_transform(titanic_test["Cabin"])
test_features = pd.DataFrame([encoded_class,
 encoded_cabin,encoded_sex,titanic_test["Age"]]).T

# Make test set predictions
test_preds = log_model.predict(X=test_features)
# Create a submission for Kaggle
submission = pd.DataFrame({"PassengerId":titanic_test["PassengerId"],
 "Survived":test_preds})
# Save submission to CSV
submission.to_csv("tutorial_logreg_submission.csv",
 index=False) # Do not save index values
print(pd)
Output:
The table shows that the model
predicted a survival chance of
roughly 19% for males and 73%
for females.

For the Titanic
competition, accuracy is
the scoring metric used
to judge the competition,
so we don't have to
worry too much about
other metrics.
PSIT1P1~~~~~ Research in Computing Practical
M. Sc. [Information Technology] SEMESTER ~ I Teacher’s Reference Manual
63
The table above shows the classes our model predicted vs. true
values of the Survived variable.
This logistic regression model has an accuracy score of 0.75598 which is actually
worse than the accuracy of the simplistic women survive, men die model (0.76555).
